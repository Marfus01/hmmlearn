开启调试模式：
```cmd
# 如果要开发，建议先fork仓库，然后克隆你的fork
git clone  https://github.com/your-username/hmmlearn.git 
cd hmmlearn

# 添加上游仓库
git remote add upstream  https://github.com/hmmlearn/hmmlearn.git 

# 开发模式安装
pip install -e .
```

经过验证，当n_trials = 1时，MultinomialHMM与CategoricalHMM等价。由于我们只会用到n_trials = 1的情况，因此后续开发将主要围绕CategoricalHMM展开。

## 参数推断
我希望通过继承 hmmlearn 中一些类和功能进行二次开发，根据以下 latex 描述的 nested HMM生成的数据：
To be concrete, Let $\cI_i=(\bF_i,\bS_i;\hat\bF_i,\hat\bS_i)$ be the joint vector of unknown latent indicators $\bX_i=(\bF_i,\bS_i)$ and observed predicted indicators $\bY_i=(\hat\bF_i,\hat\bS_i)$ by $\cM_F$ and $\cM_S$ for video $\bV_i$, and $\cI=\{\cI_i\}_{1\leq i\leq m}$.
%The dependency structure of the components in $\cI$ can be very complex and is influenced by actor relationships, plot dynamics, and the structure of the algorithms $\cM_F$ and $\cM_S$. In light of this, the study hypothesizes that $\cI$ follows a nested Markov model. 
We set the following likelihood function for the HMM:
\begin{align}
\label{eq:SPNHMM-1}
\bbP(\cI)&=\prod_{i=1}^m\left[\bbP\big(\bF_i,\bS_i;\hat\bF_i,\hat\bS_i\big)\right]
= \prod_{i=1}^m\left[\bbP(\bF_i)\cdot\bbP(\bS_i\vert\bF_i)\cdot\bbP(\hat{\bF}_i\vert\bF_i)\cdot\bbP(\hat\bS_i\vert\bS_i)\right];\\
\label{eq:SPNHMM-2}
\bbP(\bF_i)&=\prod_{\varrho\in\cP}\bbP(\bF_{i,\cdot,\varrho})
=\prod_{\varrho\in\cP}\left[\bbP(F_{i,1,\varrho})\cdot\prod_{t=2}^{n_i}\bbP(F_{i,t,\varrho}\vert F_{i,t-1,\varrho})\right]\nonumber\\
&=\prod_{\varrho\in\cP}\left[\textbf{Bernoulli}(F_{i,1,\varrho}\vert\alpha_{\varrho})\cdot\prod_{t=2}^{n_i}\bA_{\varrho}(F_{i,t-1,\varrho},F_{i,t,\varrho})\right],\\
\label{eq:SPNHMM-3}
\bbP(\bS_i\vert\bF_i)
&=\bbP(S_{i,1}\vert \bF_{i,1,\cdot})\cdot\prod_{t=2}^{n_i}\bbP(S_{i,t}\vert S_{i,t-1};\bF_{i,t,\cdot})\nonumber\\
&=\textbf{Multinomial}(S_{i,1}\vert\bbeta_{\omega_{i,1}})\cdot\prod_{t=2}^{n_i} \bA_{\omega_{i,t}}(S_{i,t-1},S_{i,t});\\
\label{eq:SPNHMM-4}
\bbP(\hat{\bF}_i\vert\bF_i)
&=\prod_{\varrho\in\cP}\bbP(\hat\bF_{i,\cdot,\varrho}\vert\bF_{i,\cdot,\varrho})=\prod_{\varrho\in\cP}\prod_{t=1}^{n_i}\bbP(\hat F_{i,t,\varrho}\vert F_{i,t,\varrho})
=\prod_{\varrho\in\cP}\prod_{t=1}^{n_i} \bB_{\varrho}(F_{i,t,\varrho},\hat F_{i,t,\varrho}),\\
\label{eq:SPNHMM-5}
\bbP(\hat\bS_i\vert\bS_i)&=\prod_{t=1}^{n_i}\bbP(\hat S_{i,t}\vert S_{i,t})
=\prod_{t=1}^{n_i} \bB_S(S_{i,t},\hat S_{i,t});
\end{align}
where
$\alpha_{\varrho}\in(0,1)$ and $\bA_{\varrho}=\{\bA_{\varrho}(\delta,\delta')\}_{\delta,\delta' \in \{0,1\}}$ are actor-specific  \emph{initial state probability} and $2\times 2$ \emph{transition matrix} of the face track of actor $\varrho$;
$\omega_{i,t}$ is a statistics summarizing the influence of the face presence vector $\bF_{i,t,\cdot}$ to speaker transition,
$\bbeta_{\omega}=\{\beta_{\omega,\varrho}\}_{\varrho\in\cP}$ with $\sum_{\rho\in\cP} \beta_{\omega,\varrho}=1$ and $\bA_{\omega}=\{\bA_{\omega}(\varrho,\varrho')\}_{\varrho,\varrho'\in\cP}$ are $\omega$-specific \emph{initial probability} and $L\times L$ \emph{transition matrix} of the speaker track given the summary $\omega$;
$\bB_\varrho=\{\bB_\varrho(\delta,\delta')\}_{\delta,\delta' \in \{0,1\}}$ is the $2\times 2$ \emph{confusion matrix} of the face recognition model $\cM_F$ for actor $\varrho$, 
$\bB_S=\{\bB_S(\varrho,\varrho')\}_{\varrho,\varrho'\in\cP}$ is the $L\times L$ \emph{confusion matrix} of the speaker recognition model $\cM_S$.
A natural model for $\bbeta_\omega$ and $\bA_\omega$ in \eqref{eq:SPNHMM-3} is to specify $\omega_{i,t}=F_{i,t,S_{i,t}}$ and let
\begin{align}
\label{eq:SPHMM-S|F-a}
\bbP(S_{i,1}\vert\bF_{i,1,\cdot})
=&\ \bbeta_{\omega_{i,1}}(S_{i,1})\propto\exp\big(\beta_{S_{i,1}}+\gamma_1\cdot F_{i,1,S_{i,1}}\big),\\
\label{eq:SPHMM-S|F-b}
\bbP(S_{i,t}\vert S_{i,t-1};\bF_{i,t,\cdot})
=&\ \bA_{\omega_{i,t}}(S_{i,t-1},S_{i,t})
\propto\exp\big(\bA_S(S_{i,t-1},S_{i,t})+\gamma_2\cdot F_{i,t,S_{i,t}}\big),
\end{align}
where $\gamma_1,\gamma_2 \geq 0$ highlights the fact that face presence of actor $\rho$ in key frame $I_{i,t}$ would increase the probability of $\rho$ to be the speaker of the key frame, and terms in $\bbeta=\{\beta_\rho\}_{\rho\in\cP}$ and $\bA_{S}=\{\bA_S(\rho,\rho')\}_{\rho,\rho'\in\cP}$, i.e., $\beta_{S_{i,1}}$ and $\bA_S(S_{i,t-1},S_{i,t})$, model the regular initial state distribution and transition probability of the speaker track solely depending on information of the audio modal.
Further define $\balpha=\{\alpha_\varrho\}_{\varrho\in\cP}$, $\cA_F=\{\bA_{\varrho}\}_{\varrho\in\cP}$, and $\cB_F=\{\bB_\varrho\}_{\varrho\in\cP}$.
We have $\btheta=(\balpha,\cA_F;\bbeta,\gamma_1,\bA_{S},\gamma_2;\cB_F,\bB_S)$ as the parameters of the proposed model. 
通过继承 hmmlearn 中一些类和功能进行二次开发。我想要实现的第一步是通过Baum-Welch algorithm 完成模型参数的推断，现在已经完成了基本的代码框架，它们对应以下数学推断的实现：
Detailed calculation obtains the following iteratively updating function for $\btheta$:
\begin{align}
\label{eq:SPMstep1}
\alpha_\varrho^{(s+1)}&=\frac{1}{m}\bbE\left[\bbN(F_{\cdot,1,\varrho}=1\vert \btheta^{(s)})\right],\\
\label{eq:SPMstep2}
\bA_{\varrho}^{(s+1)}(\delta,\delta')&=\frac{\bbE\left[\bbN(F_{\cdot,\cdot-1,\varrho}=\delta,F_{\cdot,\cdot,\varrho}=\delta' \vert \btheta^{(s)})\right]}{\sum_{\delta^\ast=0}^{1}\bbE\left[\bbN(F_{\cdot,\cdot-1,\varrho}=\delta,F_{\cdot,\cdot,\varrho}=\delta^\ast \vert \btheta^{(s)})\right]},\\
%(1\leq\varrho_i\varrho_j\leq L-1);\\
\label{eq:SPMstep3}
\bbeta^{(s+1)}, \gamma_1^{(s+1)}&=\argmax_{\bbeta,\gamma_1}\sum_{f\in\cF}\sum_{\varrho\in\cP}\log\Big(\frac{\exp(\beta_{\varrho}+\gamma_1\cdot f_{\varrho})}{\sum_{\varrho'\in\cP}\exp(\beta_{\varrho'}+\gamma_1\cdot f_{\varrho'})}\Big)\cdot\nonumber\\
&\quad\quad\quad\quad\quad\quad\quad\quad\bbE\left[\bbN(F_{\cdot,1,\cdot}=f,S_{\cdot,1}=\varrho\vert \btheta^{(s)})\right],\\
\label{eq:SPMstep4}
\bA_S^{(s+1)}, \gamma_2^{(s+1)}&=\argmax_{\bA_S,\gamma_2}\sum_{f\in\cF}\sum_{\varrho\in\cP}\sum_{\varrho'\in\cP} \log \Big(\frac{\exp(\bA_S(\varrho,\varrho')+\gamma_2\cdot f_{\varrho'})}{\sum_{\varrho^\ast\in\cP}\exp(\bA_S(\varrho,\varrho^\ast)+\gamma_2\cdot f_{\varrho^\ast})}\Big)\cdot\nonumber\\
&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\bbE\left[\bbN(F_{\cdot,\cdot,\cdot}=f,S_{\cdot,\cdot-1}=\varrho,S_{\cdot,\cdot}=\varrho'\vert \btheta^{(s)})\right],\\
\label{eq:SPMstep5}
\bB_{\varrho}^{(s+1)}(\delta,\delta')&=\frac{\bbE\left[\bbN(F_{\cdot,\cdot,\varrho}=\delta,\hat{F}_{\cdot,\cdot,\varrho}=\delta' \vert \btheta^{(s)})\right]}{\sum_{\delta^\ast=0}^{1}\bbE\left[\bbN(F_{\cdot,\cdot,\varrho}=\delta,\hat{F}_{\cdot,\cdot,\varrho}=\delta^\ast \vert \btheta^{(s)})\right]},\\
\label{eq:SPMstep6}
\cB^{(s+1)}(\varrho,\varrho')&=\frac{\bbE\left[\bbN(S_{\cdot,\cdot}=\varrho,\hat{S}_{\cdot,\cdot}=\varrho'\vert \btheta^{(s)})\right]}{\sum_{\varrho^\ast \in \cP}\bbE\left[\bbN(S_{\cdot,\cdot}=\varrho,\hat{S}_{\cdot,\cdot}=\varrho^\ast \vert \btheta^{(s)})\right]},
\end{align}
where the six expectations are defined in Eq.~\eqref{eqsm:SPNHMMExp}. In practice, Eq.~\eqref{eq:SPMstep3} and ~\eqref{eq:SPMstep4} are solved by numerical methods such as the Newton-Raphson method, since they don't have closed-form solutions.
Directly calculating these expectations is impractical, as it requires exploring all potential $\cI_i$ values, whose complexity grow exponentially with the length of video $\bV_i$.
However, linear complexity computation can be done efficiently using the Baum-Welch forward-backward algorithm~\citep{baumwelch}.

\subsection{Fast Computation of the E-Step}\label{SM:SPHMM-imp}
The parameter estimation steps of HMM-stabilized methods can be implemented using a standard Baum-Welch algorithm. To reach complexity of enumerating hidden states, we employ the standard forward-backward algorithm to compute the six expectations in Eq.~\eqref{eqsm:SPNHMMExp}. For the observation $(\hat{\bF}_i,\hat{\bS}_i)$ of $n_i$ key frames in video $i$ and parameters $\btheta^{(s)}$ of the HMM-stabilized model, we define the forward and backward variables as follows,
\begin{align}
\bbU_{i,t}(f,\varrho)&=\bbP(\hat{\bF}_{i,[n\leq t]},\hat{\bS}_{i,[n\leq t]},F_{i,t,\cdot}=f,S_{i,t}=\varrho \vert \btheta^{(s)}),\\
\bbV_{i,t}(f,\varrho)&=\bbP(\hat{\bF}_{i,[n>t]},\hat{\bS}_{i,[n> t]}\vert F_{i,t,\cdot}=f,S_{i,t}=\varrho,\btheta^{(s)}),\quad 1\leq t \leq n_i.
\end{align}
where
\begin{align}
&\hat{\bF}_{i,[n\leq t]}=(\hat F_{i,1},\cdots,\hat F_{i,t}),\hat{\bF}_{i,[n>t]}=(\hat F_{i,t+1},\cdots,\hat F_{i,n_i}),\\
&\hat{\bS}_{i,[n\leq t]}=(\hat S_{i,1},\cdots,\hat S_{i,t}),\hat{\bS}_{i,[n>t]}=(\hat S_{i,t+1},\cdots,\hat S_{i,n_i}),\quad 1\leq t \leq n_i.
\end{align}

The forward and backward variables can be computed using the following dynamic programming iteration formula,
\begin{align}
\bbU_{i,1}(f,\varrho)=&\bbP(F_{i,1,\cdot}=f)\bbP(S_{i,1}=\varrho\vert F_{i,1,\cdot}=f)\bbP(\hat F_{i,1,\cdot} \vert F_{i,1,\cdot}=f)\bbP(\hat S_{i,1}\vert S_{i,1}=\varrho),\\
\bbU_{i,t+1}(f,\varrho)=&\sum_{f'\in \cF}\sum_{\varrho'\in \cP} \bbU_{i,t}(f',\varrho')\bbP(F_{i,t+1,\cdot}=f\vert F_{i,t,\cdot}=f')\bbP(S_{i,t+1}=\varrho\vert S_{i,t}=\varrho',F_{i,t+1,\cdot}=f) \nonumber\\
&\bbP(\hat F_{i,t+1,\cdot} \vert F_{i,t+1,\cdot}=f)\bbP(\hat S_{i,t+1}\vert S_{i,t+1}=\varrho);\\
\bbV_{i,n_i}(\varrho,\varrho')=&1,\\
\bbV_{i,t-1}(f,\varrho)=&\sum_{f'\in \cF}\sum_{\varrho'\in \cP} \bbV_{i,t}(f',\varrho')\bbP(F_{i,t,\cdot}=f'\vert F_{i,t-1,\cdot}=f)\bbP(S_{i,t}=\varrho'\vert S_{i,t-1}=\varrho,F_{i,t,\cdot}=f')\nonumber \\
&\bbP(\hat F_{i,t,\cdot} \vert F_{i,t,\cdot}=f')\bbP(\hat S_{i,t}\vert S_{i,t}=\varrho').
\end{align}

Note that
\begin{align}
\bbP(\cI_i^{obs}\vert\btheta^{(s)})&=\sum_{f\in\cF}\sum_{\varrho\in\cP}\bbU_{i,n_i}(f,\varrho),\\
\bbP(\cI_i^{obs},F_{i,t,\cdot}=f,S_{i,t}=\varrho \vert\btheta^{(s)})&=\bbU_{i,t}(f,\varrho)\bbV_{i,t}(f,\varrho).
\end{align}

Denote that $\cF_{\varrho,\delta}=\{f:f\in\cF, f_{\varrho}=\delta\}, (\varrho\in\cP, \delta\in\{0,1\})$, the expectations can be computed using the following formulas,
\begin{align}
&\bbE\left[\bbN(F_{\cdot,1,\varrho}=1\vert \btheta^{(s)})\right] = \sum_{i=1}^{m} \frac{1}{\bbP(\cI_i^{obs}\vert\btheta^{(s)})} \sum_{f\in\cF_{\varrho,1}}\sum_{\varrho'\in\cP}\Big[\bbU_{i,1}(f,\varrho)\bbV_{i,1}(f,\varrho)\Big],
\\
&\bbE\left[\bbN(F_{\cdot,\cdot-1,\varrho}=\delta,F_{\cdot,\cdot,\varrho}=\delta' \vert \btheta^{(s)})\right] = \sum_{i=1}^{m}\frac{1}{\bbP(\cI_i^{obs}\vert\btheta^{(s)})}\sum_{t=2}^{n_i}\sum_{f\in\cF_{\varrho,\delta}}\sum_{f'\in\cF_{\varrho,\delta'}}\sum_{\varrho'\in\cP}\sum_{\varrho^\ast\in\cP} \Big[\bbU_{i,t-1}(f,\varrho')\bbV_{i,t}(f',\varrho^\ast)
\nonumber\\
&\quad\bbP(F_{i,t,\cdot}=f'\vert F_{i,t-1,\cdot}=f)\bbP(S_{i,t}=\varrho^\ast\vert S_{i,t-1}=\varrho',F_{i,t,\cdot}=f')\bbP(\hat F_{i,t,\cdot} \vert F_{i,t,\cdot}=f')\bbP(\hat S_{i,t}\vert S_{i,t}=\varrho^\ast)\Big],
\\
&\bbE\left[\bbN(F_{\cdot,1,\cdot}=f,S_{\cdot,1}=\varrho\vert \btheta^{(s)})\right] = \sum_{i=1}^{m}\frac{1}{\bbP(\cI_i^{obs}\vert\btheta^{(s)})}\Big[\bbU_{i,1}(f,\varrho)\bbV_{i,1}(f,\varrho)\Big],
\\
&\bbE\left[\bbN(F_{\cdot,\cdot,\cdot}=f,S_{\cdot,\cdot-1}=\varrho,S_{\cdot,\cdot}=\varrho'\vert \btheta^{(s)})\right] =\sum_{i=1}^{m} \frac{1}{\bbP(\cI_i^{obs}\vert\btheta^{(s)})}\sum_{t=2}^{n_i}\sum_{f'\in\cF}
\Big[\bbU_{i,t-1}(f',\varrho)\bbV_{i,t}(f,\varrho')
\nonumber\\
&\quad\bbP(F_{i,t,\cdot}=f\vert F_{i,t-1,\cdot}=f')\bbP(S_{i,t}=\varrho' \vert S_{i,t-1}=\varrho,F_{i,t,\cdot}=f)\bbP(\hat F_{i,t,\cdot} \vert F_{i,t,\cdot}=f)\bbP(\hat S_{i,t}\vert S_{i,t}=\varrho')\Big],
\\
&\bbE\left[\bbN(F_{\cdot,\cdot,\varrho}=\delta,\hat{F}_{\cdot,\cdot,\varrho}=\delta' \vert \btheta^{(s)})\right] = \sum_{i=1}^{m}\frac{1}{\bbP(\cI_i^{obs}\vert\btheta^{(s)})}\sum_{t=1}^{n_i}\bbI(\hat{F}_{i,t,\varrho}=\delta')\sum_{f\in\cF_{\varrho,\delta}}\sum_{\varrho'\in\cP}\Big[\bbU_{i,t}(f,\varrho')\bbV_{i,t}(f,\varrho')\Big],
\\
&\bbE\left[\bbN(S_{\cdot,\cdot}=\varrho,\hat{S}_{\cdot,\cdot}=\varrho'\vert \btheta^{(s)})\right] = \sum_{i=1}^{m}\frac{1}{\bbP(\cI_i^{obs}\vert\btheta^{(s)})}\sum_{t=1}^{n_i}\bbI(\hat{S}_{i,t}=\varrho')\sum_{f\in\cF}\Big[\bbU_{i,t}(f,\varrho)\bbV_{i,t}(f,\varrho)\Big].
\end{align}
现有实现继承了 hmmlearn 中的层次结构，并利用了 C++完成了性能优化。